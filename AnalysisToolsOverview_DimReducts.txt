
Dimensionality Reduction Algorithms:

OBS: Initial approach - compare the variance capture by the individual components with either analysis method (!) - "# components" vs. "Residual Variance"
- enables us to evaluate the applicability of either analysis approach for each dataset


OVERVIEW REVIEWED IN:
- Cunningham et Yu, 2014, Nat Neurosci.
- https://www.nature.com/articles/nn.3776

COMPARATIVE REVIEW WITH REGARD TO SEPARATION:
- https://arxiv.org/ftp/arxiv/papers/1710/1710.04484.pdf


#1 PCA

- Standard PCA

#2 GPFA (Gaussian process factor analysis)

- Yu, Cunningham et al. 2009, j.neurophys
- https://journals.physiology.org/doi/full/10.1152/jn.90941.2008

BYRON YU's CODES FOR MATLAB:
- https://users.ece.cmu.edu/~byronyu/software.shtml

"Byron Yu's Matlab code for factor analysis"
- https://github.com/mobeets/fa

"Python implementation of Gaussian Process Factor Analysis"
- https://github.com/harvineet/py-gpfa

#3 NMF (non-negative matrix factorization)
- https://www.nature.com/articles/44565

[shymkivy/ensemble_analysis_YS]
extracting ensembles with NMF
- https://github.com/shymkivy/ensemble_analysis_YS

NMF implementation based on Lee and Seung's multiplicative rules.
- https://github.com/gionuno/nonnegative_matrix_factorization

NMFLibrary: Non-negative Matrix Factorization (NMF) Library: Version 2.1
- https://github.com/hiroyuki-kasai/NMFLibrary

#4 t-SNE (t-distributed stochastic neighbor embedding)

Original SNE-paper:
- https://cs.nyu.edu/~roweis/papers/sne_final.pdf

Modified t-SNE paper:
- https://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf

- Scikit-learn, a popular machine learning toolkit in python implements t-SNE with both exact solutions and the Barnes-Hut approximation

#5 cvPCA 

- Stringer et al. 2019, Nature
- https://www.nature.com/articles/s41586-019-1346-5

- cvPCA method (see figure 2):

"The cvPCA method is fully described in Supplementary Discussion 1, characterized mathematically in Supplementary Discussion 1.1 and 3.6, 
and analysed in simulation in Extended Data Fig. 5. 

In brief, the difference between this approach and standard PCA (for example, see previous studies47,48) 
is that it compares the activity on training and test repeats to obtain an estimate of the stimulus-related (‘signal’) variance, 
discounting variance from trial-to-trial variability (‘noise’)."

In "https://github.com/MouseLand/stringer-pachitariu-et-al-2018b/blob/master/process_data.m":

% compute cross-validated PCs and signal variance and SNR and responsiveness
% saves in matroot/eigs_and_stats_all.mat
statsShuffledPCA(dataroot, matroot);

https://github.com/MouseLand/stringer-pachitariu-et-al-2018b/blob/79850dba7a4e66e213245105c00131f4d6c84e03/powerlaws/statsShuffledPCA.m

#6 RASTERMAP + SVCA

- Stringer et al. 2019, Science
- https://www.science.org/doi/10.1126/science.aav7893?cookieSet=1

- Rastermap (see Methods + Figure 1)
  - https://github.com/MouseLand/RasterMap
  - Language: Python3 OR MatLab2016a
  - Forked: https://github.com/RasHerlo/rastermap.git

- SVCA method (see Methods + Figure 1I):
  - [Shared variance component analysis]
  - "The SVCA method gives an asymptotically unbiased lower-bound estimate for the amount of a neural population’s variance reliably encoding a latent signal."

#7 ISOMAP (see Yuriv's Dim.Red.)




